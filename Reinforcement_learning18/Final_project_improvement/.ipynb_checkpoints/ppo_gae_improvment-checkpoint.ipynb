{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-14 18:59:03,305] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "# env_name = \"Pendulum-v0\"\n",
    "env_name = \"FrozenLake-v0\"\n",
    "\n",
    "# def make_env():\n",
    "#     def _thunk():\n",
    "#         env = gym.make(env_name)\n",
    "#         return env\n",
    "\n",
    "#     return _thunk\n",
    "\n",
    "# envs = [make_env() for i in range(num_envs)]\n",
    "# envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_fix(evir,states):\n",
    "    new_state = np.zeros(evir.observation_space.n)\n",
    "    new_state[states] =  1\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model,vis=False):\n",
    "    state = env.reset()\n",
    "    state = state_fix(env,state)\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = np.argmax(dist.sample().cpu().numpy()[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        state = state_fix(env,state)\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages,model, optimizer,clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs  = envs.observation_space.shape[0]\n",
    "# num_outputs = envs.action_space.shape[0]\n",
    "    \n",
    "def inizilize_prams():\n",
    "    num_inputs = env.observation_space.n\n",
    "    num_outputs = env.action_space.n\n",
    "\n",
    "    #Hyper params:\n",
    "    hidden_size      = 55#256\n",
    "    lr               = 3e-4\n",
    "    num_steps        = 17\n",
    "    mini_batch_size  = 5\n",
    "    ppo_epochs       = 4\n",
    "    threshold_reward = 10\n",
    "\n",
    "    model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    max_frames = 10000\n",
    "    frame_idx  = 0\n",
    "    test_rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state_fix(env,state)\n",
    "    done = False\n",
    "    early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def running_params(gamma_tune, tau_tune):\n",
    "\n",
    "    num_inputs = env.observation_space.n\n",
    "    num_outputs = env.action_space.n\n",
    "\n",
    "    #Hyper params:\n",
    "    hidden_size      = 55#256\n",
    "    lr               = 3e-4\n",
    "    num_steps        = 17\n",
    "    mini_batch_size  = 5\n",
    "    ppo_epochs       = 4\n",
    "    threshold_reward = 10\n",
    "\n",
    "    model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    max_frames = 10000\n",
    "    frame_idx  = 0\n",
    "    test_rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state_fix(env,state)\n",
    "    done = False\n",
    "    early_stop = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "        log_probs = []\n",
    "        values    = []\n",
    "        states    = []\n",
    "        actions   = []\n",
    "        rewards   = []\n",
    "        masks     = []\n",
    "        entropy = 0\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "        #while not done:\n",
    "            #print(\"start\")\n",
    "            state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "            dist, value = model(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            action_value = np.argmax(action.cpu().numpy()[0])\n",
    "            next_state, reward, done, _ = env.step(action_value)\n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            masks.append(1 - done)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "\n",
    "            state = next_state\n",
    "            state = state_fix(env,state)\n",
    "            frame_idx += 1\n",
    "\n",
    "            if frame_idx % 1000 == 0:\n",
    "                test_reward = np.mean([test_env(model) for _ in range(10)])\n",
    "                test_rewards.append(test_reward)\n",
    "                plot(frame_idx, test_rewards)\n",
    "                if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "        #print(\"next_state\", next_state)\n",
    "        next_state = state_fix(env,next_state)\n",
    "        next_state = torch.FloatTensor(next_state).to(device).unsqueeze(0)\n",
    "        _, next_value = model(next_state)\n",
    "\n",
    "\n",
    "        returns = compute_gae(next_value, rewards, masks, values,gamma_tune, tau_tune)\n",
    "\n",
    "        returns   = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(log_probs).detach()\n",
    "        values    = torch.cat(values).detach()\n",
    "        states    = torch.cat(states)\n",
    "        actions   = torch.cat(actions)\n",
    "        advantage = returns - values\n",
    "\n",
    "        ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage,model, optimizer)\n",
    "\n",
    "        values = []\n",
    "        policy = []\n",
    "        for state_plt in range(0,16):\n",
    "            state_plt = state_fix(env,state_plt)\n",
    "            state_plt = torch.FloatTensor(state_plt).to(device).unsqueeze(0)\n",
    "            dist, value = model(state_plt)\n",
    "            action = np.argmax(dist.sample().cpu().numpy()[0])\n",
    "            policy.append(action)\n",
    "            values.append(value)\n",
    "        #print(\"end_Values\", values)\n",
    "        #print(\"end_Policy\", policy)\n",
    "    return (policy,values, np.sum(test_rewards),(gamma_tune, tau_tune))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Running through parameters</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma_tau_opt = []\n",
    "top_rewards_9up = np.zeros((20,20))\n",
    "top_rewards = np.zeros((10,10))\n",
    "tick = -1\n",
    "\n",
    "# for gamma_tune in range(0,9):\n",
    "#     for tau_tune in range(0,9):\n",
    "#         returns = running_params(gamma_tune/10, tau_tune/10)\n",
    "#         #min_reward_place = min(top_rewards)\n",
    "#         #if top_rewards[min_reward_place] < returns[2]:\n",
    "#         top_rewards[gamma_tune][tau_tune] = returns[2]\n",
    "#         gamma_tau_opt.append(returns)\n",
    "\n",
    "eight_up = [.81,.82,.83,.84,.85,.86,.87,.88,.89,.90,.91,.92,.93,.94,.95,.96,.97,.98,.99,1]\n",
    "tock = -1\n",
    "for gamma_tune in range(81,101):\n",
    "    tick += 1\n",
    "    tock = -1\n",
    "    for tau_tune in range(81,101):\n",
    "        tock += 1\n",
    "        returns = running_params(gamma_tune/100,tau_tune/100)\n",
    "        top_rewards_9up[tick][tock] = returns[2]\n",
    "        gamma_tau_opt.append(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"gamm_tao\", gamma_tau_opt)\n",
    "#print(\"top_rewards\", top_rewards)\n",
    "plt.imshow(top_rewards, cmap='hot', interpolation='nearest')\n",
    "\n",
    "plt.imshow(top_rewards_9up_cp, cmap='hot', interpolation='nearest')\n",
    "# plt.imshow(top_rewards_9up, cmap='hot', interpolation='nearest')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
